{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T13:34:02.472238Z",
     "start_time": "2024-04-29T13:34:00.071380Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Basics\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "#Toolkits\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "# Modelling\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#Statistic tools - ACF/PACF\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "#import our Base Class\n",
    "from model_base import Prediction_Base\n",
    "#ignore warnings from matplotlib.close - is needed to avoid overflow of notebook\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#avoid matplot plots\n",
    "%matplotlib agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4c75d-eec2-43e6-8410-31af5584ca3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T13:34:17.677Z",
     "start_time": "2024-04-29T13:34:11.841346Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseNN(Prediction_Base):\n",
    "    def prepare_for_training(self):\n",
    "        \"\"\"\n",
    "        Initiates the Model, and creates the predictions with the corresponding seed given.\n",
    "        assigns rf, y_train_pred, y_test_pred\n",
    "        \"\"\"\n",
    "        self.rf = MLPRegressor(hidden_layer_sizes=(50,23),random_state=self.seed)\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        #all predictions\n",
    "        self.y_train_pred = self.rf.predict(self.X_train)\n",
    "        self.y_test_pred = self.rf.predict(self.X_test)\n",
    "        \n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Where the specific model is saved. Creates the model directory if it does not exist.\n",
    "        Assigns: Nothing\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.experimentpath + f\"saved_models/\"):\n",
    "            os.mkdir(self.experimentpath + f\"saved_models/\")\n",
    "        joblib.dump(self.rf, self.experimentpath + f\"saved_models/{self.seed}.joblib\")\n",
    "        pass\n",
    "\n",
    "    def load_model(self, model_seed):\n",
    "        \"\"\"\n",
    "        Loads the model from the directory\n",
    "        Assigns: rf\n",
    "        \"\"\"\n",
    "        self.rf = joblib.load(self.experimentpath + f\"saved_models/{model_seed}.joblib\")\n",
    "\n",
    "    def register_mse_of_model(self):\n",
    "        \"\"\"\n",
    "        compares the MSE between train and test.\n",
    "        Assigns: stats_mse\n",
    "        \"\"\"\n",
    "        self.stats_mse[f'{self.seed}'] = {\n",
    "                                            'MSE_train_scaled':mean_squared_error(self.y_train_pred,self.y_train),\n",
    "                                            'MSE_test_scaled': mean_squared_error(self.y_test_pred,self.y_test),\n",
    "                                            'MSE_cumsum_test': mean_squared_error(self.y_test, self.y_test_pred)\n",
    "                                         }\n",
    "                                                         \n",
    "\n",
    "    def save_mse_of_models(self):\n",
    "        \"\"\"\n",
    "        saves the MSE of the models in statistics folder for this model.\n",
    "        Creates the directory if not existing.\n",
    "        Assigns: Nothing\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.experimentpath + f\"statistics/\"):\n",
    "            os.mkdir(self.experimentpath + f\"statistics/\")\n",
    "        try:\n",
    "            with open(self.experimentpath+f'statistics/mse_of_models.json', 'w') as file:\n",
    "                file.write(json.dump(str(self.stats_mse), file))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def run_model_configuration(self):\n",
    "        \"\"\"\n",
    "        The main loop for the model. We set the numpy and model seed here. \n",
    "        \"\"\"\n",
    "        self.read_in_data()\n",
    "        self.raw_data.head()\n",
    "        self.plot_data_for_comparison()\n",
    "        self.acf_plot()\n",
    "        self.create_lags()\n",
    "        self.split_dataset_by_years()\n",
    "        self.split_dataset_by_train_test()\n",
    "        for i in self.random_seeds:\n",
    "            self.set_seed(i)\n",
    "            np.random.seed(i)\n",
    "            self.prepare_for_training()\n",
    "            self.return_to_original_values()\n",
    "            self.plot_performance()\n",
    "            regressors = self.append_regressors()\n",
    "            self.save_model()\n",
    "            self.register_mse_of_model()\n",
    "            self.hit_ratio()\n",
    "        self.save_mse_of_models()\n",
    "        self.plot_train_comparison()\n",
    "        self.plot_test_comparison()\n",
    "        self.plot_test_comparison_Patrick()\n",
    "        self.model_statistics_unscaled()\n",
    "        self.write_stats_to_file()\n",
    "\n",
    "years = [[2014, 2015, 2016], \n",
    "         [2015, 2016, 2017], \n",
    "         [2016, 2017, 2018], \n",
    "         [2017, 2018, 2019], \n",
    "         [2018, 2019, 2020], \n",
    "         [2019, 2020, 2021], \n",
    "         [2020, 2021, 2022], \n",
    "         [2021, 2022, 2023], \n",
    "         [2022, 2023, 2024],\n",
    "         [2014, 2015], \n",
    "         [2015, 2016], \n",
    "         [2016, 2017],\n",
    "         [2017, 2018], \n",
    "         [2018, 2019], \n",
    "         [2019, 2020], \n",
    "         [2020, 2021], \n",
    "         [2021, 2022], \n",
    "         [2022, 2023], \n",
    "         [2023, 2024],\n",
    "         [2014, 2015, 2016, 2017], \n",
    "         [2015, 2016, 2017, 2018], \n",
    "         [2016, 2017, 2018, 2019], \n",
    "         [2017, 2018, 2019, 2020], \n",
    "         [2018, 2019, 2020, 2021], \n",
    "         [2019, 2020, 2021, 2022], \n",
    "         [2020, 2021, 2022, 2023], \n",
    "         [2021, 2022, 2023, 2024]]\n",
    "\n",
    "for train_iter in years:\n",
    "        for dataset in os.listdir('../data/'):\n",
    "            print(f'Years {train_iter} on dataset {dataset} started running.')\n",
    "            if dataset == '.ipynb_checkpoints':\n",
    "                continue\n",
    "            if dataset == 'btc_hist.csv' or dataset == 'eth_hist.csv':\n",
    "                    iter_lags = [1, 7, 14, 21, 28, 'auto']\n",
    "            else:\n",
    "                    iter_lags = [1, 5, 10, 15, 20, 'auto']\n",
    "            try:\n",
    "                for lag in iter_lags:\n",
    "                        model = BaseNN(dataname = dataset, \n",
    "                                        train_years=train_iter[:-1], \n",
    "                                        test_years=[train_iter[-1]],\n",
    "                                        no_of_lag = lag,\n",
    "                                        automate_lag= True if lag == 'auto' else False,\n",
    "                                        model = 'NN'\n",
    "                                        )\n",
    "                        \n",
    "                        model.run_model_configuration()\n",
    "            except:\n",
    "                pass\n",
    "            print(f'Years {train_iter} on dataset {dataset} finished running.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcd3454e29994e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:48:55.832903Z",
     "start_time": "2024-04-29T14:48:55.773202Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class top_models(BaseNN):\n",
    "    \"\"\"\n",
    "    All methods from the model class aswell as the model_base are inherited. \n",
    "    The process includes finding the best 10 models, through the MSE of the training dataset.\n",
    "    Various plots are made, used in the paper. \n",
    "    \"\"\"\n",
    "    def load_model_statistics(self, dataname):\n",
    "        #folders to be ignored in subsequent runs\n",
    "        self.ignore_folders = ['original_comparison_plot.png', \n",
    "                               'original_comaprisonplot.png',\n",
    "                               'tex_table_top1_asset.txt']\n",
    "        \n",
    "        self.dataname = dataname\n",
    "        self.top_seeds = []\n",
    "        self.path = f'../results/{self.model}/{dataname}/'\n",
    "        self.sorted_list_with_seed = []\n",
    "        self.all_y_train_pred = {}\n",
    "        self.all_y_test_pred = {}\n",
    "        self.number_models = 10\n",
    "        self.run_top_10_loop()\n",
    "\n",
    "    def run_top_10_loop(self):\n",
    "        try:\n",
    "            for self.lag in os.listdir(self.path):\n",
    "                if self.lag in self.ignore_folders:\n",
    "                    continue\n",
    "                for self.yearfolder in os.listdir(self.path+f'{self.lag}/'):\n",
    "                        if self.yearfolder in self.ignore_folders:\n",
    "                            continue\n",
    "                        for self.contentfolder in os.listdir(self.path+f'{self.lag}/{self.yearfolder}/'): \n",
    "                            if self.contentfolder == 'statistics':\n",
    "                                with open(f'{self.path}{self.lag}/{self.yearfolder}/{self.contentfolder}/mse_of_models.json','r') as file:\n",
    "                                    self.loaded_mse_of_models = eval(json.load(file))\n",
    "                                self.write_correlation_to_file()\n",
    "                                self.discover_top_seeds()\n",
    "                                self.load_top_models()\n",
    "                                self.read_in_data()\n",
    "                                self.split_years_create_lag_mplot()\n",
    "                                self.plot_top_performance()\n",
    "                                self.plot_top_performance_train()\n",
    "                                self.hitratio_plot()\n",
    "                                self.aggregate_and_plot()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def split_years_create_lag_mplot(self):\n",
    "        \"\"\"\n",
    "        In this function, four things happen:\n",
    "        1. the train and test years are separated and evaluated into a proper list\n",
    "        2. the lags are assigned. This happens through the folder, or in case of auto, through the model parameters\n",
    "        3. the respective datasets are created in split_dataset_*\n",
    "        4. mplot_* are assigned to be used later to plot\n",
    "        \"\"\"\n",
    "        # making sure the train/testsplit is done properly\n",
    "        self.train_years = eval(self.yearfolder[:-7])\n",
    "        self.test_years = eval(self.yearfolder[-6:])\n",
    "        self.mplot_sign = []\n",
    "        self.mplot_prop = []\n",
    "        \n",
    "        #in case the lag is 'auto' as folder, the features from the provided model is taken as lag.\n",
    "        if self.lag == 'auto':\n",
    "            self.no_of_lag = self.models[next(iter(self.models))].n_features_in_\n",
    "        else:\n",
    "            self.no_of_lag = eval(self.lag)\n",
    "        #self.automate_lag = False\n",
    "        self.acf_plot()\n",
    "        self.create_lags()\n",
    "\n",
    "        \n",
    "        self.split_dataset_by_years()\n",
    "        self.split_dataset_by_train_test()\n",
    "        self.prediction_data_for_plot_scaled = {}\n",
    "        for list_entry in self.sorted_list_with_seed:\n",
    "            for seed in list_entry:\n",
    "                if int(seed) >= 1:\n",
    "                    self.rf = self.models[seed]\n",
    "                    self.predict(seed)\n",
    "                    perf_sign = np.cumsum(self.y_test * np.sign(self.y_test_pred))  # Simplified\n",
    "                    perf_prop = np.cumsum(self.y_test * self.y_test_pred / np.mean(self.y_test_pred))\n",
    "        \n",
    "                    self.mplot_sign.append(perf_sign)\n",
    "                    self.mplot_prop.append(perf_prop)           \n",
    "\n",
    "    def create_lags(self):\n",
    "            \"\"\"\n",
    "            This function creates the lags, as decided in the function acf_plot2. \n",
    "            It creates a new dataset called self.lags.\n",
    "            This dataset consists of the last X days, where X = number of lags.\n",
    "            The values are shifted, after a diff and log transformation.\n",
    "            It also isolates the first values of raw data. Its value is the same as the first value of train\n",
    "            The lag columns are named after their day, increasing with distance.\n",
    "            \"\"\"\n",
    "            print(f'-----------------#of lag created: {self.lag}------------------')\n",
    "            self.first_value_raw_data = self.raw_data[\"Close\"].iloc[0]\n",
    "            self.raw_after_diff_and_log = pd.DataFrame(np.diff(np.log(self.raw_data[\"Close\"])),\n",
    "                                               index = self.raw_data.index[1:], columns=['Close'])\n",
    "            \n",
    "            self.lags = [self.raw_after_diff_and_log.shift(i) for i in range(self.no_of_lag+1)]\n",
    "            #make self.lags into a dataframe again\n",
    "            self.lags = pd.concat(self.lags, axis=1)\n",
    "            self.lags.columns = ['Lag_'+str(i) for i in range(self.no_of_lag+1)]\n",
    "            self.lags = self.lags.dropna()\n",
    "\n",
    "    def discover_top_seeds(self, number_models = 10):\n",
    "        \"\"\"\n",
    "        Based on the train MSE, we discover the best available seeds.\n",
    "        If the number of models is lower than the requested amount, all models are returned to compensate.\n",
    "        The models are sorted, lowest MSE first.\n",
    "\n",
    "        Assigns: sorted_list_with_seed, top_seeds\n",
    "        \"\"\"\n",
    "        self.top_seeds = []\n",
    "\n",
    "        for seed in self.loaded_mse_of_models:\n",
    "            self.top_seeds.append((seed, self.loaded_mse_of_models[seed]['MSE_train_scaled']))\n",
    "        try:\n",
    "            self.sorted_list_with_seed = sorted(self.top_seeds, key = lambda x: x[1], reverse = False)[:number_models]\n",
    "        except:\n",
    "            self.sorted_list_with_seed = sorted(self.top_seeds, key = lambda x: x[1], reverse = False)\n",
    "\n",
    "    def load_top_models(self):\n",
    "        \"\"\"\n",
    "        Based on the finding in discover_top_seeds, the best models are loaded for further use. \n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        for seed, mse in self.sorted_list_with_seed:\n",
    "            self.models[seed] = (joblib.load(self.path + f\"{self.lag}/{self.yearfolder}/saved_models/{seed}.joblib\"))\n",
    "\n",
    "    def predict(self, seed):\n",
    "        \"\"\"\n",
    "        Predictions are made and saved in the respective variables. This can differ, directly for the model or for the different seeds, denoted by all_*\n",
    "        \"\"\"    \n",
    "        self.y_train_pred = self.rf.predict(self.X_train)\n",
    "        self.y_test_pred = self.rf.predict(self.X_test)\n",
    "        self.all_y_train_pred[seed] = self.y_train_pred\n",
    "        self.all_y_test_pred[seed] = self.y_test_pred\n",
    "\n",
    "\n",
    "    def plot_top_performance(self):\n",
    "        \"\"\"\n",
    "        In this function, the top cumulative performance is plotted. \n",
    "\n",
    "        Assigns: Nothing\n",
    "\n",
    "        Plot saved under: \"../images/RandomForest/btc_hist.csv/auto/[2022,2023]-[2024]/Top-Model-PerformancePlots/Best_Performances_[2022,2023]-[2024]_auto.png\"\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.path +f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/'):\n",
    "            os.mkdir(self.path +f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/')\n",
    "        \n",
    "        if len(self.y_test_pred) != len(self.y_test):\n",
    "            raise ValueError(\"y_test_pred and target_out length mismatch. Check for leap year issues.\")\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in self.models:\n",
    "            plt.plot(pd.DataFrame(np.cumsum(np.sign(self.models[i].predict(self.X_test))*self.y_test), index=self.X_test.index))\n",
    "        plt.plot(pd.DataFrame(np.cumsum(self.y_test),index=self.X_test.index), label='Buy & Hold', color = 'black', linewidth=2)\n",
    "        plt.legend()\n",
    "        plt.title(f\"{self.model}, Lag {self.no_of_lag}\", fontdict = {'fontsize' : 30})\n",
    "        plt.ylabel('Cumulative Performance', fontsize=15)\n",
    "        plt.xticks(fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n",
    "        plt.savefig(self.path + f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/Best_Performances_{self.yearfolder}_{self.lag}.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_top_performance_train(self):\n",
    "        \"\"\"\n",
    "        In this function, the top cumulative performance for train is plotted. \n",
    "\n",
    "        Assigns: Nothing\n",
    "\n",
    "        Plot saved under: \"../images/RandomForest/btc_hist.csv/auto/[2022,2023]-[2024]/Top-Model-PerformancePlots/Best_Performances_[2022,2023]-[2024]_auto.png\"\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.path +f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/'):\n",
    "            os.mkdir(self.path +f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/')\n",
    "        \n",
    "        if len(self.y_test_pred) != len(self.y_test):\n",
    "            raise ValueError(\"y_test_pred and target_out length mismatch. Check for leap year issues.\")\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in self.models:\n",
    "            plt.plot(pd.DataFrame(np.cumsum(np.sign(self.models[i].predict(self.X_train))*self.y_train), index=self.X_train.index))\n",
    "        plt.plot(pd.DataFrame(np.cumsum(self.y_train),index=self.X_train.index), label='Buy & Hold', color = 'black', linewidth=2)\n",
    "        plt.legend()\n",
    "        plt.title(f\"{self.model}, Lag {self.no_of_lag}\", fontdict = {'fontsize' : 30})\n",
    "        plt.ylabel('Cumulative Performance', fontsize=15)\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.savefig(self.path + f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/Best_Performances_{self.yearfolder}_{self.lag}_train.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def hitratio_plot(self):\n",
    "        \"\"\"\n",
    "        We create a plot using the Hitratios provided by the models. The models are chosen by train MSE, see above for more details.\n",
    "        The plot of the best 10 models is then saved.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.path +f'{self.lag}/{self.yearfolder}/Top-Hitratio/'):\n",
    "            os.mkdir(self.path +f'{self.lag}/{self.yearfolder}/Top-Hitratio/')\n",
    "        hitratios = {}\n",
    "        hitratio_values_for_boxplot_test = []\n",
    "        for list_entry in self.sorted_list_with_seed:\n",
    "                 for seed in list_entry:\n",
    "                    if int(seed) >= 1:\n",
    "                        with open(self.path + f'{self.lag}/{self.yearfolder}/hitratio/{seed}','r') as file:\n",
    "                            hitratios[seed] = eval(json.load(file))\n",
    "        \n",
    "        for seed in hitratios:\n",
    "            #plots the hitratio as a regular plot.\n",
    "            plt.plot(hitratios[seed]['Hit Ratio Test'], label = seed, marker='o')\n",
    "            #appends values for test\n",
    "            hitratio_values_for_boxplot_test.append(hitratios[seed]['Hit Ratio Test'])\n",
    "        plt.legend()\n",
    "        plt.savefig(self.path + f'{self.lag}/{self.yearfolder}/Top-Hitratio/Best-Hitratios-Test_{self.yearfolder}_{self.lag}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        ## Boxplot for Hitratios\n",
    "        pd.DataFrame(hitratio_values_for_boxplot_test, columns=[f'Hit-Ratios {self.dataname.split(\"_\")[0]}']).boxplot(grid=False, fontsize=15)\n",
    "        plt.savefig(self.path + f'{self.lag}/{self.yearfolder}/Top-Hitratio/Best-Hitratios-Test_Boxplot_{self.yearfolder}_{self.lag}.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_actual_performance(self):\n",
    "        \"\"\"\n",
    "        Legacy, not further used. \n",
    "        \"\"\"\n",
    "        y_test_pred=pd.DataFrame(self.y_test_pred, index=self.y_test.index)\n",
    "        plt.plot(y_test_pred, label='Prediction')\n",
    "        plt.plot(self.y_test, label='True', color = 'black', linewidth = 2)\n",
    "        #plt.legend()\n",
    "        plt.savefig(self.path + f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/Best_Performances_{self.yearfolder}_{self.lag}_Actual_Performance.png')\n",
    "        plt.close()\n",
    "        \n",
    "    def write_correlation_to_file(self):\n",
    "        \"\"\"\n",
    "        Pearson Correlation is written into a file. For each model-lag-year combination, the MSE correlation is written into a file. \n",
    "        \"\"\"\n",
    "        mse_train = []\n",
    "        mse_test = []\n",
    " \n",
    "        # Extract MSE values from the loaded JSON data\n",
    "        for seed, mse_values in self.loaded_mse_of_models.items():\n",
    "            mse_train.append(mse_values['MSE_train_scaled'])\n",
    "            mse_test.append(mse_values['MSE_test_scaled'])\n",
    "        \n",
    "        # Results in a 2x2 Matrix, we're only interested in Pearson Corr [0,1]\n",
    "        correlation = np.corrcoef(mse_train, mse_test)[0, 1]\n",
    "        \n",
    "        with open(self.path +f'{self.lag}/{self.yearfolder}/statistics/mse_corr.json', \"w\") as file:\n",
    "            json.dump(round(correlation,6), file)\n",
    "        \n",
    "    def aggregate_and_plot(self):\n",
    "        \"\"\"\n",
    "        The best 10 seeds are used to create a plot that uses the mean (eg. the most recommended course of action) for the current date. \n",
    "        \n",
    "        \"\"\"\n",
    "        # Convert lists to numpy arrays for easier manipulation\n",
    "        mplot_sign = np.column_stack(self.mplot_sign)\n",
    "        mplot_prop = np.column_stack(self.mplot_prop)\n",
    "        # print(mplot_sign)\n",
    "        # Aggregate performance across all models\n",
    "        self.perf_agg_sign_mse = np.mean(mplot_sign, axis=1)\n",
    "        self.perf_agg_prop_mse = np.mean(mplot_prop, axis=1)\n",
    "        # Prepare data for plotting\n",
    "        cumulative_y_test = np.cumsum(self.y_test)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(cumulative_y_test, label='Buy & Hold', color='black', linewidth = 2)\n",
    "        plt.plot(pd.DataFrame(self.perf_agg_sign_mse, index=self.X_test.index), label='Sign Aggregate', color='red')\n",
    "        #plt.plot(pd.DataFrame(self.perf_agg_prop_mse, index=self.X_test.index), label='Proportional Aggregate', color='blue')\n",
    "        plt.title(f'Aggregate Performances Over {self.number_models} Seeds')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Cumulative Performance')\n",
    "        plt.legend()\n",
    "        plt.savefig(self.path + f'{self.lag}/{self.yearfolder}/Top-Model-PerformancePlots/Aggregated_Performances_{self.yearfolder}_{self.lag}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef25d2d0cfe6a8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:49:09.412156Z",
     "start_time": "2024-04-29T14:48:56.281016Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "               \n",
    "for i in os.listdir('../data/'):\n",
    "    if i in ['.ipynb_checkpoints', 'tex_table_top1_asset.txt']:\n",
    "        continue\n",
    "    try:\n",
    "        t10 = top_models(model='NN', dataname=i)\n",
    "        print(f'Processing dataset {i}')\n",
    "        t10.load_model_statistics(dataname = i)\n",
    "        t10.load_top_models()\n",
    "    except:\n",
    "        pass\n",
    "    print(f'Finished dataset {i}')\n",
    "\n",
    "\n",
    "#images/RandomForest/btc_hist.csv/[2014, 2015]-[2016]/saved_models/1.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821dfe1c-311d-42c3-9aa0-73e930293b41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
